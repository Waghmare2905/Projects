Application 2 to create a model or to create a model which is going to predict whether a wafer is faulty or not so this is a bigger problem we have broken down it into many smaller problems for example the first small problem will be how to read the data once we have the data how to validate that data after data validation how to do data preprocessing steps how to train a model on it how to do parameter tuning for a model and so on all those things are the smaller steps that we have broken our steps into and then by combining all these steps we are going to give the bigger solution so the question is why there are two answers to it actually so the first thing is that if you break your problem into smaller problems right so you can focus on one thing at a time and it will be easier to solve your problem as well as it will be easy to maintain also if you have a change in any one module you do not need to change the entire code instead of that you will be changing On how your team has been divided so different people they might get different modules as well so this is the reason we have broken down or takes it to different bodies now lets see what are the modules that we have after start the first thing that you see right from here still here this is our radiation module or the data ingestion pipeline what we are doing here is that we are reading the training batches or we are reading the files which are kept at a file location this is the task of your client the client is going to place the files on the training files at a certain shared address and from that shared file location you are going to read it now once you have read the files there are certain validations that we need to do before the starting of the project we must have had a conversation with our client that this is going to be a data type for the columns these are going to be the number of columns none of the columns they should contain all values as null all these validations sorry all these these arrangements or agreements we would have made with our client so now we need to validate those things as well in our data so those kind of validations whether a number of columns they are correct or not whether the data types for individual columns they are correct or not whether there is any column which contains all the null values whether that kind of a column is present or not all these kind of validations it will be done here in the data validation step after that we need to do data transformation now what kind of a data transformation so there are some things But there are some rooms which we need to follow before inserting into our database for example if the missing values are presented in our data we need to explicitly convert them to null so that our database should accept it first thing second thing is for categorical values there might be scenarios that your client is sending data at close in single inverted comma but there can be scenarios that your database is not going to accept it instead of that it will expect you insert your category values in close inside double inverted comma so all these kind of transformations we are doing and then we are inserting whatever data we have so in batch files right we can have north number of batch all these north number of batch file data will be aggregated and then it will be stored into your database now this is the aggregation of all the N batch files Now once this has been done right what we are going to do is we are going to create a single CSV file which will be a combination of all the valid data from this patch file this is what we are going to do so now this particular step this is now going to act as an input for the subsequent steps once we have our data so this is our training data on this trending data we are going to perform exponentially data analysis and data preprocessing in data preprocessing we are going to see whether like how can we convert the categorical values into numerical values whether there are any null values present so we need to handle that as well whether the data is imbalanced or not whether the data is normalized or not all these things whatever you are aware of what data frequency all these things we are going to do here in our data represent steps once our data is clean we are happy with it what we are going to do is we are now going to do clustering of our data why are we doing clustering clustering we are doing to increase the accuracy of individual models that we are going to prepare so what we are going to do is we are not going to create a single model for the entire data set first we will try to find out some implicit patterns in the data by dividing them into different clusters or different globes now once we have divided our data into different clusters or different groups these cluster numbers they will be added to the printing data so you will be having one more column which will consist of the cluster number after that what we are going to do is we will be looking over these individual clusters we will segregate the data and loop over those clusters and then what we are going to do is we are going to prepare individual models for individ Once we have finalized that this model is working fine for this cluster then comes the step of hyper parameter tuning we will tune the individual models for individual clusters for a better accuracy and once we have that hyperparameter tuning done we will save the model through this entire thing right from this data preprocessing till model saving it can be included as your training pipeline or training step so this entire thing combined together they are called as model training now after this what we need to do is once our model has been trained we need to deploy our model to cloud so for deploying our model to cloud we need certain pre request that is based on the cloud environment in which we are trying to deploy for example TCP will have a different set of requirements azure they will have a different set of requirements so there are some metadata files that we need to create before deploying to the corresponding cloud environment and once those things are created once those metadata files are created then what we are going to do is then we are finally going to create an app on that cloud that we are going to push our application to the cloud after that our application it will get automatically started by the cloud server itself and what's the application start what we are going to do is our application is ready for prediction but the thing is whatever data the client is giving we are not going to directly give that data for prediction why because as you have seen here in the training or data ingestion step itself there can be scenarios that whatever you agreed upon for data types that kind of that kind of a thing it is not followed by all the files so those kind of validations we need to again do whenever we are doing the prediction so we are going to perform similar set of validations what we did for our training step we will perform all those validation with the number of columns are correct or not but the data types for corresponding columns are correct or not again we are going to do data transformations for null values there should be for a missing values they should be replaced with null and similar inverted commas they will be replaced by double inverted commas then again we will create all the we will combine everything whatever prediction data our client is sending so there will be north number of prediction files right so all those north prediction files they will be combined together and they will be started into a database Again from this database we are going to export a CSV file and now this is going to act as our prediction again whatever data preprocessing steps that we did for training whether a missing value whether converting the category values to numerical values and so on all those data preprocessing steps we are going to perform on our prediction data as well and again our data will pass through the clustered algorithm passing our data to the clustering algorithm what it's going to do is it going to give us the cluster number that OK this particular data it belongs to this cluster so this clustering it will be done based on the saved model and now we will get the cluster number so now what is the model which has been trained for that cluster using that particular model for that specific cluster we are going to make the actual predictions and this will go on for all the clusters that we obtain into the prediction data once we add done with the prediction on all all the data that our client has provided we are going to combine all those things and we are going to store it into a CSV file and now from here your client can again pole and read the data and whatever is the subsequent step in their pipeline the client's pipeline what to do with this prediction data whether to change a wafer when to change a wafer how to monitor all these things that will be taken care by your client one more thing that we need to do is there will be model retraining approaches as well so whenever you are training a model it is not that one model is trained under this final it is not like that whenever there are new patterns occurring in the client data then we need to aggregate those patterns into our pre trained model also whatever we have played previously so we have we have that thing aggregated into our model also this is again where this data database will come into picture So this data this database it will contain the previous data now we are going to append the new rows whatever new patterns we are observing those new patterns they will be appended here and again we will export a CSV file so this CSV file now it will contain old data as well as the new data provided by the client and hence whatever our learning was previously we are not going to use that learning and on top of that we are going to learn things new things as well so this is again one more advantage of having a master database sort of thing it will quite helpful when you are doing the model readering and again after all these things once you have deployed your model and all you need a logging framework as well and avoiding framework as well so that you can continuously see what is the logs whether your applications are they are working properly or not whether they are any failures or not if there are any failures what kind of failures you are able to see all those things you need to monitor it for the that we will have a login framework as well this was our application architecture let us talk about our flow as well what is going to be the flow of our application let's talk about that as so the floor is going to be like this let me open a new file and let me clear all the rotations so the control will first come to your main dot PY and from this mail dot P we have broadly classified push steps so first step is going to be valuation step what we are going to do is we are going to read data from whatever location the client is placing the files we are going to read those files then we are going to perform certain validations for example whether the number of columns are correct or not whether the data types come from the columns are correct or not all those things after that we will transform the data according to our database needs and whatever columns we are specified and then we are going to insert the data once we are inserting the data we will export everything to a CSV file after this this exported CSE file is now the input of our second step this is going to be the input of our second step what is our second step the control will now come to the second step called as training in training what we are going to do is again from this CSV file exported CSV file we are going to read data read training data and after that we will do data preprocessing on it type rotation of missing value is the conversion of categorical values that you numerical values the balancing of a data set the normalization of the data values all those things we are going to perform here once this part is done we will perform a we will form a cluster then we will find out individual models for individual clusters that perform well so model finding out after that we will fine tune the models model tuning step and finally we will be able ready to deploy it and once validation is done and the training is done same may not be Y can be again used for prediction as well what we are going to do is we will define different route one route will be for training and the other route is going to be for prediction so in training route there will be validation step and the actual model training step again in prediction route as well there will be a validation step and there will be a prediction step in the prediction step what we are going to do is validation step is going to be similar to whatever we have all these steps we are again going to perform and in prediction step what we are going to do is whatever models we have saved after doing data pre processing whatever models we have saved those models we will be loading into our memory and using those models we will be trying to predict what is the outcome for a particular whether that wafer is in a working condition or whether that vapor is in is not in a working condition so this is going to be our application architecture and application